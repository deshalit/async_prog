Зробив два рішення.
1. "Line indexes method"
    Те, що потрібно було зробити по завданню. Тобто в процеси-воркери передаються стартовий та фінальний індекси
    в глобальному масиві рядків. Помітив підвищений расход пам'яті при роботі :(
2. "Offset + count method"
    Ідея полягає в тому, що ми передаємо в процеси-воркери ім'я файлу, стартову позицію для читання та кількість рядків
    для читання. Воркер відкриває файл для читання, робить seek() на вказану позицію, далі в циклі робить
    readline() вказану кількість разів.
    Як ми отримали кількість рядків та стартові позиції для кожного воркера: функція prepare()
    бере загальний розмір файлу, розбиває його на кількість ядер процесора (кількість воркерів). Отримали
    chunk_size - приблизний розмір для перевірки одним воркером.
    Далі читаємо файл, враховуючи чудову функціональність readlines(chunk_size) - вона повертає стільки рядків, скільки
    містить chunk_size. Запам'ятали поточну позицію та кількість рядків.
    Таким чином ми для кожного воркер-процесу отримали стартову позицію та кількість рядків.

В головному файлі words.py можна обрати потрібний метод - закоментувати зайвий.

Параметри:
* ім'я файлу (обов'язковий перший параметр)
* слово для рахування (необов'язковий, за замовчанням це "ära"